---
title: "Untitled"
author: "brunop31"
date: "26/06/2020"
output: html_document
---
```{r, warning=FALSE}
library(ggplot2)
library(dplyr)
library(factoextra)
library(FactoMineR)
library(corrplot)
library(caret)
library(MASS)
library(car)
library(nnet)
library(devtools)
library(randomForest)


select<-dplyr::select

```

```{r}
df_all<-read.csv(
  "donnee_dendris.csv",
  encoding = "UTF-8", sep = ",", dec = ".")

df_all<-df_all[,-1]

which(is.na(df_all), arr.ind = TRUE)

df_all[195,11]<-median(filter(df_all, Souche == "Spn")[,11],na.rm = TRUE)

df_all<-df_all[-182,]

row.names(df_all)<-1:263

which(is.na(df_all), arr.ind = TRUE)

###on supprime les sondes trop faibles
sonde_force<-apply(df_all%>%select(-"Souche"), 2, sum)
which(sonde_force < 5000)
df_all<-df_all[,-(which(sonde_force < 5000)+1)]

###on supprime les sans variance
var_0<-which(apply(select(df_all,-"Souche"), 2, var)==0)%>%
  as.data.frame()%>%select(num = ".")%>%mutate(sonde = row.names(.))

df_all<-select(df_all, -var_0$sonde)
```

```{r}
###je supprime les variables trop corrélées
dfs_all<-df_all%>%select(-"Souche")
cormat <- cor(dfs_all, method = "pearson")%>%as.data.frame()

which(abs(cormat)>0.8, arr.ind = TRUE)%>%as.data.frame()%>%
  filter(row != col)%>%filter(
    !(col %in% c(28,29,30,33,38,42,50,1,24,26,43,12,13,17,55,58,59,6,20,
                 7,48,10,11,54,45,19,39,57)))%>%
  filter(!(row %in%c(28,29,30,33,38,42,50,1,24,26,43,12,13,17,55,58,59,6,
                     20,7,48,10,11,54,45,19,39,57)))

#which.max(sonde_force[colnames(df_all)[c(28,29,30,33,38,42,43,50,1)+1]])

df<-dfs_all%>%
  select(-c(28,29,30,33,38,42,50,1,24,26,43,12,13,17,55,58,59,6,20,
                 7,48,10,11,54,45,19,39,57))

df<-cbind(df_all$Souche, df)%>%rename("Souche" = "df_all$Souche")

dfs<-df%>%select(-"Souche")

###Je scale les données
dfs_scale<-dfs%>%scale()%>%as.data.frame()

df_scale<-dfs_scale%>%cbind(df$Souche, .)%>%rename("Souche" = "df$Souche")
```


```{r}
pred<-data.frame()
for (i in df$Souche%>%levels()) {
  x<-filter(df_scale[,1:33], Souche == i)%>%select(-"Souche")
  iso<-isotree::isolation.forest(x, ntrees = 100, nthreads = 1,
                                 ndim =1)
  y<-predict(iso, x)%>%as.data.frame()
  y$Souche<-i
  pred<-pred%>%rbind(y)
}
```

```{r}
###Recherche outlier

b1 = c()
b2 = c()
b3 = c()
b4 = c()

c = c()

i = 1

for (i in 1 : 10) {

################# Fit the model3 Neural network##########################
model3 <- nnet(Souche ~., df_scale, size = 10,
               rang = 0.5, subset = 1:263,
            decay = 5e-60, maxit = 10, MaxNWts = 100000, abstol = 1)

predictions<-model3 %>% predict(df_scale)

x<-colnames(predictions)[apply(predictions, 1, which.max)]

b3<-c(b3, x)

################# Fit the model4 Random forest###########################
model4 <- randomForest(Souche ~., df_scale, subset = 1:263, 
                       xtest = dfs_scale,
                       ytest = df_scale$Souche)


b4<-c(b4, model4$predicted%>%as.vector())

#############################################################
c<-c(c,df_scale$Souche%>%as.vector())

i = i+1
}

error3<-which(b3 != c)%>%mod(263)%>%as.data.frame()%>%
  rename("samp" = "." )%>%group_by(samp)%>%
  count()

error4<-which(b4 != c)%>%mod(263)%>%as.data.frame()%>%
  rename("samp" = "." )%>%group_by(samp)%>%
  count()

error34<-full_join(error3,error4, by = "samp")%>%
  rename("nnet" = "n.x", "rf" = "n.y")

error34[is.na(error34)]<-0
error34$somme<-error34$rf + error34$nnet

error34$samp[1]<-263
vote<-model4$votes
```


```{r}
#######################sonde normale###################################
pvalue<-c()
sonde<-c()
for (i in 2:61) {
 x<-shapiro.test(df_all[,i])
  pvalue[i-1]<-x$p.value
  sonde[i-1]<-colnames(df_all)[i]
}

gauss<-cbind(sonde,pvalue)%>%data.frame()
gauss$pvalue<-gauss$pvalue%>%as.vector()%>%as.numeric()
normal_sonde<-gauss%>%arrange(desc(pvalue))%>%.[1:9,1]%>%as.vector()

dfs_normal<-df_all[,colnames(df_all) %in% normal_sonde]%>%
  as.data.frame()

dfs_normal$somme<-df_all%>%select(-"Souche")%>%apply(1,sum)%>%scale()%>%
  as.numeric()

df_normal<-cbind(df$Souche, dfs_normal)%>%rename("Souche" = "df$Souche")

model1 <- klaR::rda(Souche~., data = df_normal)
predictions <- model1 %>% predict(df_normal)

confusionMatrix(predictions$class, df_normal$Souche)
```

```{r}
###Recherche outlier isolation forest
pred<-data.frame()
for (i in df$Souche%>%levels()) {
  x<-filter(df_normal[,1:11], Souche == i)%>%select(-"Souche")
  iso<-isotree::isolation.forest(x, ntrees = 100, nthreads = 1,
                                 ndim =1)
  y<-predict(iso, x)%>%as.data.frame()
  y$Souche<-i
  pred<-pred%>%rbind(y)
}
```

```{r}
###Recherche outlier

b1 = c()
b2 = c()

c = c()

i = 1

for (i in 1 : 10) {

################### Fit the model1 LDA ###########################
model1 <- lda(Souche~., data = df_normal)
# Make predictions
predictions <- model1 %>% predict(df_normal)

b1<-c(b1, predictions$class%>%as.vector())

################### Fit the model2 RDA ###########################
model2 <- klaR::rda(Souche~., data = df_normal)
# Make predictions
predictions <- model2 %>% predict(df_normal)

b2<-c(b2, predictions$class%>%as.vector())

#############################################################
c<-c(c,df_scale$Souche%>%as.vector())

i = i+1
}

conf1<-confusionMatrix(b1%>%as.factor(), c%>%as.factor())
conf2<-confusionMatrix(b2%>%as.factor(), c%>%as.factor())
conf1
conf2

error1<-which(b1 != c)%>%mod(263)%>%as.data.frame()%>%
  rename("samp" = "." )%>%group_by(samp)%>%
  count()

error2<-which(b2 != c)%>%mod(263)%>%as.data.frame()%>%
  rename("samp" = "." )%>%group_by(samp)%>%
  count()


error12<-full_join(error1,error2, by = "samp")%>%
  rename("lda" = "n.x", "rda" = "n.y")

error12[is.na(error12)]<-0
error12$somme<-error12$lda + error12$rda

error12$samp[1]<-263


```



```{r}
###J'effectue une PCA
df_pca<-PCA(dfs_normal, graph = FALSE)

###J'observe le pourcentage de variance expliquée de chacun de mes
###axes obtenue par pca
fviz_eig(df_pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r}
###cercle de corrélation du premier plan
fviz_pca_var(df_pca)
```

```{r}
###premier plan (1 et 2) 
fviz_pca_ind(df_pca,geom.ind = "point", col.ind = df$Souche)

#########################################################################
```

```{r}
##########################################################################
###On scale les lignes
df_normal_rs<-df_normal%>%select(-"Souche",-"somme")%>%
  apply(1, scale)%>%t()%>%
  as.data.frame()%>%cbind(df_all$Souche,.)

df_normal_rs[is.na(df_normal_rs)]<-0

df_normal_rs<-cbind(df_normal_rs, df_normal$somme)

colnames(df_normal_rs)<-colnames(df_normal)

dfs_normal_rs<-df_normal_rs%>%select(-"Souche")

###je supprime les variables trop corrélées
cormat <- cor(dfs_normal_rs, method = "pearson")%>%as.data.frame()

which(abs(cormat)>0.55, arr.ind = TRUE)%>%as.data.frame()%>%
  filter(row != col)%>%filter(
    !(col %in% c()))%>%
  filter(!(row %in%c()))

```


```{r}
###J'effectue une PCA
df_pca<-PCA(dfs_normal_rs, graph = FALSE)

###J'observe le pourcentage de variance expliquée de chacun de mes
###axes obtenue par pca
fviz_eig(df_pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r}
###cercle de corrélation du premier plan
fviz_pca_var(df_pca)
```

```{r}
###premier plan (1 et 2) 
fviz_pca_ind(df_pca,geom.ind = "point", col.ind = df$Souche,
             position=position_dodge(width=0.3))

#########################################################################
```
```{r}
#Je fais un kmeans clustering
k11<-kmeans(dfs_normal_rs, 11, 25)

fviz_cluster(k11, dfs_normal_rs,
show.clust.cent = TRUE, # Show cluster centers
ggtheme = theme_minimal(),
main = "Factor map"
)
```

```{r}
###Recherche outlier isolation forest
pred<-data.frame()
for (i in df$Souche%>%levels()) {
  x<-filter(df_normal_rs[,1:11], Souche == i)%>%select(-"Souche")
  iso<-isotree::isolation.forest(x, ntrees = 100, nthreads = 1,
                                 ndim =1)
  y<-predict(iso, x)%>%as.data.frame()
  y$Souche<-i
  pred<-pred%>%rbind(y)
}
```

```{r}
k11$cluster%>%as.vector()

mean(df$Souche%>%as.numeric()==factor(
  k11_row_scale$cluster,labels = c(9,5,4,1,3,8,11,7,10,2,6))%>%
  as.vector()%>%as.numeric())

predictions<-factor(
  k11_row_scale$cluster,labels =
    df$Souche%>%as.factor()%>%levels()%>%.[c(9,5,4,1,3,8,11,7,10,2,6)])%>%
  as.vector()%>%as.factor()

confusionMatrix(df$Souche%>%as.factor(),
                predictions)
#Le kmeans ne différencie pas les xneg les xneg et les pa/ les spn et mpn
```


```{r}
###J'effectue une PCA
df_pca<-PCA(dfs, graph = FALSE)

###J'observe le pourcentage de variance expliquée de chacun de mes
###axes obtenue par pca
fviz_eig(df_pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r}
###cercle de corrélation du premier plan
fviz_pca_var(df_pca)
```

```{r}
###premier plan (1 et 2) 
fviz_pca_ind(df_pca,geom.ind = "point", col.ind = df$Souche)
```

```{r}
#Je fais un kmeans clustering
k11<-kmeans(dfs_scale, 11, 25)

fviz_cluster(k11, dfs_scale,
show.clust.cent = TRUE, # Show cluster centers
ggtheme = theme_minimal(),
main = "Factor map"
)
```

```{r}
factor(k11$cluster)

mean(df$Souche%>%as.numeric()==factor(
  k11$cluster,labels = c(4,7,1,5,9,8,3,11,2,6,10))%>%
  as.vector()%>%as.numeric())
    

#Le kmeans ne différencie pas bien les xneg, ssp, pa et les hi 
```

```{r}
model1 <- lda(Souche~., data = df_scale)
predictions <- model1 %>% predict(df_scale)

confusionMatrix(predictions$class, df_scale$Souche)

error<-which(predictions$class != df_scale$Souche, arr.ind = TRUE)
```

```{r}
#Je supprime les xneg
df_xneg<-df_scale%>%filter(Souche != "Xneg")
df_xneg$Souche<-df_xneg$Souche%>%as.vector()%>%as.factor()


model1 <- lda(Souche~., df_xneg)
predictions <- model1 %>% predict(df_xneg)

confusionMatrix(predictions$class, df_xneg$Souche)
```

```{r}
model1 <- mda::mda(Souche~., data = df_scale)
predictions <- model1 %>% predict(df_scale)

confusionMatrix(predictions, df_scale$Souche)

error_mda<-which(predictions != df_scale$Souche, arr.ind = TRUE)

error[error %in% error_mda]
```

```{r}
model1 <- mda::fda(Souche~., data = df_scale)
predictions <- model1 %>% predict(df_scale)

confusionMatrix(predictions, df_scale$Souche)
```

```{r}
model1 <- klaR::rda(Souche~., data = df_scale)
predictions <- model1 %>% predict(df_scale)

confusionMatrix(predictions$class, df_scale$Souche)

error_rda<-which(predictions$class != df_scale$Souche, arr.ind = TRUE)

error[error %in% error_rda]
```

```{r}
##########################test scale ligne###############################

###On calcul les sommes par echantillon
somme<-dfs%>%apply(1,sum)%>%as.data.frame()%>%rename("sum" = ".")
somme$samp<-row.names(somme)%>%as.numeric()
somme<-somme%>%arrange(sum)
row.names(somme)<-1:263
somme$rank<-row.names(somme)

error<-full_join(error12, error34, by = "samp")
error[is.na(error)]<-0
error$somme<-error$somme.x+error$somme.y
error<-inner_join(error, somme, by = "samp")
```

```{r}
###On scale les lignes
df_row_scale<-df_all%>%select(-"Souche")%>%
  apply(1, scale)%>%t()%>%
  as.data.frame()%>%cbind(df_all$Souche,.)

df_row_scale[is.na(df_row_scale)]<-0

colnames(df_row_scale)<-colnames(df_all)

df_row_scale$somme<-df_all%>%select(-"Souche")%>%apply(1,sum)%>%scale()

###je supprime les variables trop corrélées des row_scale
dfs_row_scale<-df_row_scale%>%select(-"Souche")
cormat <- cor(dfs_row_scale, method = "pearson")%>%as.data.frame()

which(abs(cormat)>0.8, arr.ind = TRUE)%>%as.data.frame()%>%
  filter(row != col)%>%filter(
    !(col %in% c(28,1,4,7,23,2,26,30,58,6,60,22,48,10,11,
                 12,55,24,24,45,8,27,18,42,38)))%>%
  filter(!(row %in% c(28,1,4,7,23,2,26,30,58,6,60,22,48,10,11,
                 12,55,24,24,45,8,27,18,42,38)))

df_row_scale<-dfs_row_scale%>%
  select(-c(28,1,4,7,23,2,26,30,58,6,60,22,48,10,11,
                 12,55,24,24,45,8,27,18,42,38))

df_row_scale<-cbind(df_all$Souche, df_row_scale)%>%
  rename("Souche" = "df_all$Souche")

dfs_row_scale<-df_row_scale%>%select(-"Souche")
```

```{r}
###J'effectue une PCA
df_pca<-PCA(dfs_row_scale, graph = FALSE)

###J'observe le pourcentage de variance expliquée de chacun de mes
###axes obtenue par pca
fviz_eig(df_pca, addlabels = TRUE, ylim = c(0, 50))
```

```{r}
###cercle de corrélation du premier plan
fviz_pca_var(df_pca)
```

```{r}
###premier plan (1 et 2) 
fviz_pca_ind(df_pca,geom.ind = "point", col.ind = df$Souche)
```



```{r}
#Je fais un kmeans clustering
k11_row_scale<-kmeans(dfs_row_scale, 11, 25)

fviz_cluster(k11_row_scale, dfs_row_scale,
show.clust.cent = TRUE, # Show cluster centers
ggtheme = theme_minimal(),
main = "Factor map"
)
```

```{r}
factor(k11_row_scale$cluster)

mean(df$Souche%>%as.numeric()==factor(
  k11_row_scale$cluster,labels = c(9,5,4,1,3,8,11,7,10,2,6))%>%
  as.vector()%>%as.numeric())

predictions<-factor(
  k11_row_scale$cluster,labels =
    df$Souche%>%as.factor()%>%levels()%>%.[c(9,5,4,1,3,8,11,7,10,2,6)])%>%
  as.vector()%>%as.factor()

confusionMatrix(df$Souche%>%as.factor(),
                predictions)
#Le kmeans ne différencie pas les xneg les xneg et les pa/ les spn et mpn
```
```{r}
#fit lda model
model1 <- lda(Souche~., data = df_row_scale)
predictions <- model1 %>% predict(df_row_scale)

confusionMatrix(predictions$class, df_row_scale$Souche)

model1 <- klaR::rda(Souche~., data = df_row_scale)
predictions <- model1 %>% predict(df_row_scale)

confusionMatrix(predictions$class, df_row_scale$Souche)

##########################################################################
```

```{r}
df_out<-df%>%filter(!(rownames(df) %in% c(95,96,263)))

b1 = c()
b2 = c()
b3 = c()
b4 = c()
b5 = c()

c = c()

i = 1

for (i in 1 : 10) {

training.samples <- df_out$Souche %>%
  createDataPartition(p = 0.7, list = FALSE)

samp<-training.samples[,1]

train.data <- df_out[training.samples, ]

test.data <- df_out[-training.samples, ]

var_0<-which(apply(select(train.data,-"Souche"), 2, var)==0)%>%
  as.data.frame()%>%select(num = ".")%>%mutate(sonde = row.names(.))

train.data<-select(train.data, -var_0$sonde)
test.data<-select(test.data, -var_0$sonde)

# Estimate preprocessing parameters
preproc.param <- train.data %>% 
  preProcess(method = c("center", "scale"))
# Transform the data using the estimated parameters
train.transformed <- preproc.param %>% predict(train.data)
test.transformed <- preproc.param %>% predict(test.data)

df.transformed<-rbind(train.transformed,test.transformed)

################### Fit the model1 LDA ###########################
model1 <- lda(Souche~., data = train.transformed)
# Make predictions
predictions <- model1 %>% predict(test.transformed)

b1<-c(b1, predictions$class%>%as.vector())

################### Fit the model2 RDA ###########################
model2 <- klaR::rda(Souche~., data = train.transformed)
# Make predictions
predictions <- model2 %>% predict(test.transformed)

b2<-c(b2, predictions$class%>%as.vector())

################# Fit the model3 Neural network##########################
model3 <- nnet(Souche ~., df.transformed, size = 10,
               rang = 0.5, subset = samp,
            decay = 5e-30, maxit = 50, MaxNWts = 100000, abstol = 1)

predictions<-model3 %>% predict(test.transformed)

x<-colnames(predictions)[apply(predictions, 1, which.max)]

b3<-c(b3, x)


################# Fit the model4 Random forest###########################
model4 <- randomForest(Souche ~., df.transformed, subset = samp, 
                       xtest = test.transformed%>%select(-"Souche"),
                       ytest = test.transformed$Souche)


b4<-c(b4, model4$test$predicted%>%as.vector())

########################multinom ################################
model5 <- multinom(Souche ~., train.transformed)

b5<-c(b5, predict(model5, test.transformed)%>%as.vector())


#############################################################
c<-c(c,test.data$Souche%>%as.vector())

i = i+1
}
conf1<-confusionMatrix(b1%>%as.factor(), c%>%as.factor())
conf2<-confusionMatrix(b2%>%as.factor(), c%>%as.factor())
conf3<-confusionMatrix(b3%>%as.factor(), c%>%as.factor())
conf4<-confusionMatrix(b4%>%as.factor(), c%>%as.factor())
conf5<-confusionMatrix(b4%>%as.factor(), c%>%as.factor())

conf1
conf2
conf3
conf4
conf5
```





